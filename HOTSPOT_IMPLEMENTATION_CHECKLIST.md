# âœ… HOTSPOT NARRATION IMPLEMENTATION CHECKLIST

## Overview
Verifies that the hotspot narration system is implemented exactly like `auto_voice.py` with proper fallback chain.

---

## ğŸ“‹ COMPONENT CHECKLIST

### âœ… 1. Local LLM Configuration
**File**: `narration.py` (Lines 26-32)
```python
LOCAL_LLM_URL = os.getenv("RC_LOCAL_LLM_URL", "http://127.0.0.1:8000/v1/chat/completions")
MIMIC3_TTS_URL = os.getenv("RC_MIMIC3_TTS_URL", "http://127.0.0.1:59125/api/tts")
HOTSPOT_AUDIO_DEVICE = os.getenv("RC_HOTSPOT_AUDIO_DEVICE", "plughw:0,0")
```
**Status**: âœ… Configured  
**Match to auto_voice.py**: Yes - Uses same URLs and ALSA device pattern

---

### âœ… 2. Hotspot Detection
**Function**: `is_hotspot_mode()` (Lines 35-50)
- Checks wlan0 IP range: `192.168.4.x` or `10.42.0.x`
- Returns True if in hotspot mode, False otherwise

**Status**: âœ… Implemented  
**Notes**: Used to auto-switch to local LLM + Mimic3 when no internet

---

### âœ… 3. Internet Connectivity Check
**Function**: `has_internet_connectivity()` (Lines 53-61)
- Optional DNS test for explicit internet verification
- Currently not used but available for future enhancements

**Status**: âœ… Implemented  
**Usage**: Can be called for explicit connectivity checks

---

### âœ… 4. Local LLM Integration
**Function**: `describe_image_local_llm()` (Lines 64-118)
- Accepts JPEG bytes (camera frame)
- Encodes as base64 for transmission
- Sends to local LLM endpoint
- Supports model selection (default: `llama-2-vision`)

**Status**: âœ… Implemented  
**Match to auto_voice.py**: Uses same HTTP POST pattern with JSON payload

**Implementation Details**:
```
POST http://127.0.0.1:8000/v1/chat/completions
{
    "model": "llama-2-vision",
    "messages": [{"role": "user", "content": [...image_content...]}],
    "temperature": 0.7,
    "max_tokens": 100
}
```

---

### âœ… 5. Mimic3 TTS Integration (CRITICAL - Matches auto_voice.py EXACTLY)
**Function**: `_play_mimic3_tts()` (Lines 121-171)

**Implementation EXACTLY like auto_voice.py**:
```python
# Step 1: GET request to Mimic3 API (streaming)
r = requests.get(MIMIC3_TTS_URL, params={"text": text}, stream=True, timeout=15)

# Step 2: Pipe directly to aplay ALSA device
aplay = subprocess.Popen(
    ['aplay', '-D', audio_device, '-q'],
    stdin=subprocess.PIPE
)
aplay.communicate(input=r.content)
```

**Status**: âœ… Implemented  
**Match to auto_voice.py**: âœ… EXACT - Uses streaming GET + aplay pattern

---

### âœ… 6. NarrationEngine Configuration
**Class**: `NarrationEngine` (narration.py)

#### Init Variables (Lines 486-497):
- `_local_llm_url`: URL for local LLM endpoint
- `_use_local_llm`: Flag to enable local LLM mode
- `_mimic3_enabled`: Flag to enable Mimic3 TTS for hotspot
- `_mimic3_url`: Mimic3 API endpoint
- `_mimic3_audio_device`: ALSA device for Mimic3 output

**Status**: âœ… All variables added

#### Configuration Method (Lines 565-585):
**Function**: `set_local_llm_config(url=None, force_use=False)`
- Sets local LLM URL
- Enables hotspot mode detection
- **Auto-enables Mimic3 TTS when in hotspot mode** âœ…

**Status**: âœ… Implemented with Mimic3 auto-enable

---

### âœ… 7. Main Narration Loop Changes
**Method**: `_loop()` (Lines 690-760)

#### New Logic Flow for Hotspot Mode:
```
1. Detect hotspot mode via is_hotspot_mode()
2. IF hotspot OR forced: Use LOCAL LLM (not Gemini)
3. ELSE: Try Gemini first, fall back to LOCAL LLM if fails
4. IF got text: Play audio via priority chain:
   - PRIMARY: Mimic3 TTS (if hotspot mode enabled) â† NEW EXACT auto_voice.py
   - SECONDARY: Kokoro TTS (online mode)
   - TERTIARY: Local TTS (espeak-ng fallback)
```

**Status**: âœ… Implemented in _loop()

---

### âœ… 8. Audio Playback Priority (HOTSPOT MODE - Matches auto_voice.py)
**File**: `narration.py` (Lines 730-760)

**Playback Chain**:
1. **Mimic3 TTS** (hotspot mode) - Uses `_play_mimic3_tts()`
   - HTTP GET to Mimic3 API
   - Stream to aplay
   - EXACTLY like auto_voice.py
2. **Kokoro TTS** (online mode) - Uses `_play_kokoro_with_volume()`
3. **Local TTS** (fallback) - Uses tts_local (espeak-ng)

**Status**: âœ… Chain properly ordered

---

### âœ… 9. LLM Selection Logic (Gemini vs Local LLM)
**File**: `narration.py` (Lines 705-720)

```python
# If hotspot or forced: Use LOCAL LLM only
if in_hotspot or use_local_llm:
    text = describe_image_local_llm(...)
    
# Else: Try Gemini first
else:
    text = describe_image(api_key, model_name, jpeg_bytes, prompt)
    
    # If Gemini fails: Fall back to LOCAL LLM
    if not text:
        text = describe_image_local_llm(...)
```

**Status**: âœ… Implemented with proper fallback

---

## ğŸ”— Integration with main.py
**File**: `main.py` (Line 545)
```python
narration_engine.set_local_llm_config(force_use=get_hotspot_status())
```

**Status**: âœ… Auto-detection on startup

---

## ğŸ“Š Configuration Summary

### Environment Variables Available:
```bash
# Local LLM endpoint
export RC_LOCAL_LLM_URL="http://127.0.0.1:8000/v1/chat/completions"

# Mimic3 TTS endpoint
export RC_MIMIC3_TTS_URL="http://127.0.0.1:59125/api/tts"

# ALSA audio device for Mimic3
export RC_HOTSPOT_AUDIO_DEVICE="plughw:0,0"
```

### Default Values:
- Local LLM: `http://127.0.0.1:8000/v1/chat/completions`
- Mimic3 TTS: `http://127.0.0.1:59125/api/tts`
- Audio Device: `plughw:0,0` (HiFiBerry DAC)

---

## ğŸ§ª Testing Checklist

### Quick Verification:
```bash
# Run the test script
python3 test_hotspot_narration.py
```

**Expected Output**:
- âœ… Mimic3 TTS: OK
- âœ… Local LLM (text-only): OK
- âœ… Local LLM (vision mode): OK (or appropriate fallback)
- âœ… Hotspot detection: Working
- âœ… NarrationEngine config: OK

---

## ğŸ“ Logs to Expect

### When Hotspot Mode is Detected:
```
ğŸŒ [Narration] Hotspot mode detected or local LLM forced - using local LLM
ğŸ™ï¸ [Narration] Hotspot mode detected - enabling Mimic3 TTS
ğŸ§  [Narration] Local LLM configured - URL: ..., Mode: hotspot
ğŸ™ï¸ [Narration] Mimic3 TTS: enabled
```

### When Narration Occurs (Hotspot Mode):
```
ğŸ‘ï¸ [Narration] Local LLM says: <narration text>
ğŸ™ï¸ [Narration] Mimic3 enabled (hotspot mode) - using Mimic3 TTS...
ğŸ™ï¸ [Narration] Mimic3: synthesizing...
ğŸ™ï¸ [Narration] Mimic3: playing to plughw:0,0...
âœ… [Narration] Mimic3 playback: SUCCESS
```

### When Gemini Fails (Online Mode):
```
âŒ Gemini error: 503 DNS resolution failed...
âš ï¸ [Narration] Gemini failed or returned empty - falling back to local LLM
ğŸ‘ï¸ [Narration] Local LLM says: <narration text>
ğŸ¤ [Narration] Kokoro: IP=..., Voice=...
âœ… [Narration] Kokoro playback: SUCCESS
```

---

## âœ¨ Key Implementation Details - Matches auto_voice.py EXACTLY

### 1. Mimic3 Request Pattern âœ…
```python
# EXACT MATCH to auto_voice.py lines 44-47
r = requests.get(TTS_API_URL, params={"text": text}, stream=True)
aplay = subprocess.Popen(['aplay', '-D', AUDIO_DEVICE, '-q'], stdin=subprocess.PIPE)
aplay.communicate(input=r.content)
```

### 2. Local LLM Pattern âœ…
Uses HTTP POST with streaming (compatible with ollama/llama.cpp endpoints)

### 3. Error Handling âœ…
- Mimic3 connection failure -> Try next method
- Mimic3 timeout -> Try next method
- Local LLM failure -> Try next method
- All failures -> Signal error callback

### 4. Audio Device Management âœ…
- Uses ALSA device names (e.g., `plughw:0,0`)
- Configurable via environment variables
- Consistent with HiFiBerry DAC setup

---

## ğŸ¯ Fallback Chain Summary

### ONLINE MODE (With Internet):
```
Camera Frame
    â†“
Try Gemini API
    â”œâ”€ SUCCESS â†’ Play via Kokoro TTS
    â””â”€ FAIL â†’ Try Local LLM
              â”œâ”€ SUCCESS â†’ Play via Kokoro
              â””â”€ FAIL â†’ Play via espeak-ng (fallback)
```

### HOTSPOT MODE (No Internet):
```
Camera Frame
    â†“
Use Local LLM (skip Gemini)
    â”œâ”€ SUCCESS â†’ Play via Mimic3 TTS â† AUTO_VOICE.PY PATTERN
    â””â”€ FAIL â†’ Play via Kokoro or espeak-ng
```

---

## ğŸ“Œ Summary Status
- âœ… Hotspot detection: READY
- âœ… Local LLM integration: READY
- âœ… Mimic3 TTS (exact auto_voice.py pattern): READY
- âœ… Fallback chain: READY
- âœ… NarrationEngine config: READY
- âœ… main.py integration: READY
- âœ… Error handling: READY
- âœ… Testing script: READY

**Status: FULLY IMPLEMENTED AND READY FOR TESTING** ğŸ’—
